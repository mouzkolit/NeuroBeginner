{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f1ba036",
   "metadata": {},
   "source": [
    "# Analysis of Single Cell RNA Sequencing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903ceae",
   "metadata": {},
   "source": [
    "DZ, 27.01.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0690fb9",
   "metadata": {},
   "source": [
    "Please note that this tutorial is strongly simplified to provide a brief overview about sequential RNAseq analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d79af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings should be always considered and therefore only turned off exceptionally.\n",
    "# The improvement of the readability of this tutorial might be such an exception ;) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268d1fc",
   "metadata": {},
   "source": [
    "### Step 1: Load data and understand the structure\n",
    "<ol>\n",
    "    <li>Import the raw count table as pandas dataframe, therefore import the pandas package first. It's quite typical to name it pd.</li>\n",
    "    <li>Use the pandas function read_csv </li>\n",
    "    <li> Once the csv file was read succesfully, you can display its content. Be careful. These files can be quite large. <br/>Therefore, to get an idea of the size of the input data one can use the shape attrbiute\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb85c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ae92b7",
   "metadata": {},
   "source": [
    "To understand what the data look like it's usually enough to display the first 10 rows. Print the first 10 rows of the table using the head() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433951a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1310f2a1",
   "metadata": {},
   "source": [
    "It is also possible to see the last 10 rows by using the tail function. Print the last 10 rows of the table using the tail() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7d536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "534cd69a",
   "metadata": {},
   "source": [
    "A very typical characteristic of the single cell sequencing results is the non-normal distribution of gene counts per cell. <br/>\n",
    "To display the data distribution, we will count the number of gene counts per cell and store them in a list.<br/>\n",
    "<ol>\n",
    "    <li> Create a list with sums of all row values per column (== gene counts per cell) using .sum().values </li>\n",
    "    <li> To get the maximum and minimum within a list, numpy comes with implemented functions. Therefore we have to import numpy too.</li>\n",
    "     <li>  Print the maximum and the minimum count within the new created list using np.max() and np.min().</li>\n",
    "<li> To finally plot the histogramm, we can use seaborn plotting package which comes with this specific histogramm function sns.histplot(). <br/> Therefore, import seaborn as sns and use the histplot() function. </li> </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122b319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0878ca74",
   "metadata": {},
   "source": [
    "## 2. Preprocess Your Data\n",
    "We start the preprocessing our data by keeping only cells with a biological reasonable amount of gene counts 500-3500, all other cells will be removed.\n",
    "If you modify the raw data it's quite clever to work with a copy to avoid reloading of the .csv files if you make an unintented mistake (happens to the best ;) ).\n",
    "\n",
    "<ol>\n",
    "    <li>create variables minimum_number_of_counts and maximum_number_of_counts which should hold the manual selected thresholds</li> \n",
    "    <li> copy the dataframe into a new dataframe</li>\n",
    "    <li> write a for loop to go through the df column-wise. check if the count of the column exceeds the threshold. <br/>Remove the column by using the drop() function. Make sure to overwrite the df.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf55ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_adjusted_data = data.copy()\n",
    "\n",
    "#for column in count_adjusted_data.columns:\n",
    "#        if (count_adjusted_data[column].sum() > maximum_number_of_counts) or (count_adjusted_data[column].sum() < minimum_number_of_counts) :\n",
    "#            count_adjusted_data = count_adjusted_data.drop(column, axis = 1)\n",
    "            \n",
    "#print(\"Number of cells in the raw data: \", data.shape[1])\n",
    "#print(\"Number of cells in the processed data: \", count_adjusted_data.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d07938",
   "metadata": {},
   "source": [
    "Lets see how the data distribution looks like after cells were removed from the analysis. Plot again the data distribution as done before with the sns.histplot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d45adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e98a1031",
   "metadata": {},
   "source": [
    "### Normalization is very important !\n",
    "Data Normalization is very important for all further downstream analysis and compromises two steps. <br/> \n",
    "(1): To make counts comparable among cells, we need to normalize for the total count. This is also called library-size-correction. Normalize each cell by total counts over all genes (sum), so that every cell has the same total count after normalization. <br/>\n",
    "(2) In later steps we will apply different mathematical functions that require normal distribution. Therefore log-transform our data in this step. Log- transformation serves as a variance stabilisation and approximates a normal distribution.\n",
    "One can do log-normalization by hand as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280356d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_log = count_adjusted_data.apply(lambda x: np.log(((x/sum(x))*10000)+1))\n",
    "#sns.histplot((data_log.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c295046",
   "metadata": {},
   "source": [
    "## The Scanpy Package\n",
    "So far, we just handled a basic dataframe from a csv file. This was not RNAseq specific.\n",
    "When it comes to more RNAseq specific analysis, similar to Seurat Package in R, one can use scanpy package in python. It provides many ready-to-use implemented functions for RNAseq analysis. Scanpy works with a so called annotation data object (anndata). This can be easily created from our pandas raw data frame.\n",
    "\n",
    "<ol>\n",
    "    <li> load the scanpy package</li>\n",
    "    <li>load the anndata package</li>\n",
    "    <li>create a new anndata object called 'scanpy_object' using the function ad.AnnData()</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a5d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e107ba",
   "metadata": {},
   "source": [
    "Instead of calculating the gene counts per cell using the sum() function, scanpy allows to quickly plot for genes with the highest expression. \n",
    "\n",
    "Use the scanpy plot function highest_expr_genes() and plot the top 20 genes with the function sc.pl.highest_expr_genes()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448cf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab86943",
   "metadata": {},
   "source": [
    "In parallel to previous filtering steps, scanpy allows data filtering too.\n",
    "\n",
    "<ol>\n",
    "    <li> Use scanpy preprocessing function sc.pp.filter_cells() and only keep cells with more than 500 (min_counts) genes and less than 3500 (max_counts) genes</li> <li>  double check the filtering results. are there differents ? </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaedd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4d6cd54",
   "metadata": {},
   "source": [
    "Use the scanpy preprocessing pipeline to normalize logscale the data and replot the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7578ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.pp.normalize_total(scanpy_object, target_sum=1e4)\n",
    "#sc.pp.log1p(scanpy_object)\n",
    "#count_list = scanpy_object.to_df().T.sum()\n",
    "#sns.histplot(count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00f433",
   "metadata": {},
   "source": [
    "## 3. Dimensional Reduction Methods\n",
    "\n",
    "In this tutorial, we will compare linear principal component analysis (PCA) and non-linear t-SNE dimensional reduction method results.\n",
    "\n",
    "### PCA: \n",
    "Principal component analysis compromises data standardization, covariance matrix calculation, eigenvector and eigenvalue determination, principal component selection and finally the transformation of the input data. \n",
    "Scanpy already provides a read to use pca function sc.pp.pca(). <br/> \n",
    "<ol>\n",
    "    <li> Scale your data first using sc.pp.scale() </li>\n",
    "    <li> Run PCA on your scanpy object using svd_solver='arpack' using sc.tl.pca().  </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab78efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbb503b8",
   "metadata": {},
   "source": [
    "Visualize the different components using sc.pl.pca( ... , components = ['1,2','3,4','5,6','7,8'], ncols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339d915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d252243",
   "metadata": {},
   "source": [
    "Plot the Variance Ratio using sc.pl.pca_variance_ratio(... , log=True, n_pcs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5aa1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46c83c",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "Create a t-SNE plot using scanpy using sc.tl.tsne(). You could try to measure the time by importing time package and use time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a55c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5167a004",
   "metadata": {},
   "source": [
    "Plot the t-SNE graph and view the changes in the AnnData object using sc.pl.tsne(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff83bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1dbe8a",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "Create a UMAP using sc.tl.umap(). Therefore, you have to create the neighborhood graph first using sc.pp.neighbors()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83213d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669af861",
   "metadata": {},
   "source": [
    "Plot the UMAP using sc.pl.umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75808c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39bd9e90",
   "metadata": {},
   "source": [
    "### Cluster and Compare the Plots\n",
    "Since we already calculated the neighborhood graph, we can apply a clustering algorithm to identify related points (cells) and compare the results with our 3 dimensional reduction methods. \n",
    "<ol>\n",
    "    <li>Use sc.tl.leiden() to cluster your data  </li>\n",
    "    <li>Plot PCA results using sc.pl.pca  </li>\n",
    "    <li>Plot t-SNE results using sc.pl.tsne()</li>\n",
    "    <li>Plot UMAP using sc.pl.umap() </li>\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6ca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroscience_beginner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b29e83ee11e7f59cb38c31a3bdf4f8395742e175511a4c2dd46f06d53a300255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
